{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/bbc_science_and_climate_articles_2010-2023.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use NLP to detect location in titles and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert title and description to string\n",
    "df['title'] = df['title'].astype(str)\n",
    "df['description'] = df['description'].astype(str)\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Extract location entities\n",
    "#GPE = geopolitical entities (cities/countries)\n",
    "#LOC = non-GPE, mountain ranges/bodies of water\n",
    "\n",
    "def extract_locations(text):\n",
    "    doc = nlp(text)\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
    "    return ', '.join(locations)\n",
    "\n",
    "# Apply the function to the 'Text' column and create a new column 'Locations' with the results\n",
    "df['nlp_title'] = df['title'].apply(extract_locations)\n",
    "df['nlp_descrip'] = df['description'].apply(extract_locations)\n",
    "\n",
    "#Save file with parsed locations as a csv file (it takes 7 min to rerun the code)\n",
    "df.to_csv(\"../data/articles_locations_parsed.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge with US city locality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset rows that have location data\n",
    "#10,172 out of 20,102 total rows have location data\n",
    "df1 = df[df['nlp_title'].notna() | df['nlp_descrip'].notna()]\n",
    "\n",
    "#Import US cities dataframe\n",
    "us_locations = pd.read_csv(\"../data/uscities.csv\")\n",
    "\n",
    "#Use only city\n",
    "city_list = us_locations['city'].unique().tolist()\n",
    "\n",
    "#Find titles that match location list\n",
    "def match_city(row):\n",
    "    if row['nlp_title'] in city_list:\n",
    "        return row['nlp_title']\n",
    "    elif row['nlp_descrip'] in city_list:\n",
    "        return row['nlp_descrip']\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "\n",
    "#Create column that identifies which city matches\n",
    "df1['city_match'] = df1.apply(match_city, axis=1)\n",
    "\n",
    "#Subset if data frame has at least one city_match\n",
    "df2 = df1[df1['city_match'].notna()]\n",
    "\n",
    "#Keep only important columns\n",
    "df3 = df2[['title', 'link', 'description', 'published', 'city_match']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop cities that have duplicate keys (same city name but different locality)\n",
    "us_cities_filtered = us_locations[us_locations['city'].duplicated(keep=False) == False]\n",
    "\n",
    "#Keep important columns\n",
    "us_cities_filtered = us_cities_filtered[['city', 'state_name', 'county_name', 'lat', 'lng']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename column\n",
    "df3 = df3.rename(columns = {'city_match':'city'})\n",
    "\n",
    "#Merge df4 with us_cities_filtered based on \n",
    "df4 = df3.merge(us_cities_filtered, on='city', how='inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove if city matches a country name or if it's a planet name (there are articles with Mars and Earth as a location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a list of world country names\n",
    "countries_info = []\n",
    "for country in pycountry.countries:\n",
    "    countries_info.append({\n",
    "        'country_name': country.name,\n",
    "        'alpha_2': country.alpha_2,\n",
    "        'alpha_3': country.alpha_3,\n",
    "        'official_name': getattr(country, 'official_name', country.name)  # Some countries might not have an official_name attribute.\n",
    "    })\n",
    "\n",
    "# Creating a DataFrame from the list of dictionaries.\n",
    "df_countries = pd.DataFrame(countries_info)\n",
    "\n",
    "countries_as_lists = {col_name: df_countries[col_name].tolist() for col_name in df_countries.columns}\n",
    "country_name = countries_as_lists['country_name']\n",
    "planet_name = ['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4[~df4['city'].isin(country_name)]\n",
    "final_df = df5[~df5['city'].isin(planet_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "535"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Isle', 'England', 'Rugby', 'Sardinia', 'Tripoli', 'Russia',\n",
       "       'Edinburgh', 'Oklahoma', 'Stirling', 'North Wales', 'Mount Etna',\n",
       "       'Gulf', 'North Sea', 'New York', 'Queensland', 'North Pole',\n",
       "       'Farmland', 'Chicago', 'Bolivia', 'Hereford', 'New Orleans',\n",
       "       'Northumberland', 'Perth', 'Munich', 'Amazonia', 'Sun',\n",
       "       'Johannesburg', 'Belfast', 'Birdsong', 'Suffolk', 'Patagonia',\n",
       "       'Seattle', 'Island', 'Santiago', 'Tennessee', 'Herculaneum',\n",
       "       'Truro', 'Cape Canaveral', 'San Francisco', 'San Cristobal',\n",
       "       'Brussels', 'Hampshire', 'Everest', 'Wimbledon', 'Louisiana',\n",
       "       'Gulf Stream', 'Zephyr', 'Midwest', 'Brisbane', 'Donegal',\n",
       "       'Dumfries', 'Maine', 'Edmonton', 'North East', 'Whitehaven',\n",
       "       'Nile', 'Carmichael', 'Virgin', 'Amanda', 'Marine', 'Tulsa',\n",
       "       'Baird', 'Ohio', 'Theresa', 'West End'], dtype=object)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['city'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"../data/articles_locations_coordinates.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
