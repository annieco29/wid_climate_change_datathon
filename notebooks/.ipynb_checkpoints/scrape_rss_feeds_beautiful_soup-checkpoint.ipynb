{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e674b27-35d6-4dbd-ad83-191b88a7f8c9",
   "metadata": {},
   "source": [
    "## Use Beautiful Soup to Scrape data from RSS feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29b76fe9-eaa4-4f10-bffd-4ca3f443935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a43c419-2d7f-4b2e-b3fc-429308cdee7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /Users/anniecondon/anaconda3/envs/wid_datathon_2023/lib/python3.9/site-packages (4.9.3)\n"
     ]
    }
   ],
   "source": [
    "# install lxml to parse xml code in RSS sites\n",
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f5a039-0da0-4a26-9f5d-fff9ca81ed1a",
   "metadata": {},
   "source": [
    "The **new_directory_path** below contains all the paths of the historical RSS feeds by date from the BBC climate and science located here: https://feeds.bbci.co.uk/news/science_and_environment/rss.xml?edition=uk.  \n",
    "\n",
    "In order to run this code, you can download the historical RSS feeds by installing the waybackpack package: https://github.com/jsvine/waybackpack  \n",
    "by running ```pip install waybackpack``` in your terminal.\n",
    "Then, run the command ```waybackpack https://feeds.bbci.co.uk/news/science_and_environment/rss.xml?edition=uk -d [your path] --from-date 2010``` in your terminal, selecting a specific file path on your local computer where you want to download the RSS feed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9f3af3-7510-45e8-99e4-d9f08bd5debc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path ../../../../Downloads/bbc_rss/.DS_Store/feeds.bbci.co.uk/news/science_and_environment is not a directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "class XMLAppender(object):\n",
    "    \"\"\"This class appends the separate .xml files from each respective folder.\n",
    "\n",
    "    The functions in this class loop through all of the .xml files from the listed file paths,\n",
    "    parse the data in each file, and append the data into one string so that the content of the\n",
    "    .xml files can be parsed all at once for article titles, links, descriptions, and dates.\n",
    "    \"\"\"\n",
    "    def __init__(self, *filenames):\n",
    "        assert len(filenames) > 0, 'No filenames!'\n",
    "        self.filenames = filenames\n",
    "\n",
    "    def append(self):\n",
    "        \"\"\"Loop through files and read content of each .xml file. Append all content to one \n",
    "        string object.\"\"\"\n",
    "        appended_content = \"\"\n",
    "        for filename in self.filenames:\n",
    "            xml_content = self.read_xml_file(filename)\n",
    "            appended_content += xml_content\n",
    "        return appended_content\n",
    "\n",
    "    def read_xml_file(self, filename):\n",
    "        \"\"\"Open the .xml file and read the content\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as file:\n",
    "                return file.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading XML file '{filename}': {e}\")\n",
    "            return \"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define the root directory containing date-based subdirectories\n",
    "    root_directory = '../../../../Downloads/bbc_rss'\n",
    "    branch_directory = 'feeds.bbci.co.uk/news/science_and_environment'\n",
    "    \n",
    "    # Initialize a list to store XML data from all files\n",
    "    all_xml_paths = []\n",
    "    \n",
    "    # Iterate through subdirectories by date\n",
    "    for date_directory in os.listdir(root_directory):\n",
    "        #print(date_directory)\n",
    "        date_directory_path = os.path.join(root_directory, date_directory)\n",
    "        new_directory_path = os.path.join(date_directory_path, branch_directory)\n",
    "    \n",
    "        # Check if the item is a directory\n",
    "        if os.path.isdir(new_directory_path):\n",
    "            # Iterate through XML files within the date directory\n",
    "            for xml_file_name in os.listdir(new_directory_path):\n",
    "                if xml_file_name.endswith('.xml'):\n",
    "                    xml_file_path = os.path.join(new_directory_path, xml_file_name)\n",
    "                    all_xml_paths.append(xml_file_path)\n",
    "        else:\n",
    "            print(f\"Path {new_directory_path} is not a directory.\")\n",
    "\n",
    "    # Create an instance of XMLAppender and append the XML files\n",
    "    appender = XMLAppender(*all_xml_paths)\n",
    "    appended_xml = appender.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0af299f-6433-4e18-b49d-33a9b8f53082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the various attributes of the article\n",
    "def getArticles(articles):\n",
    "    \n",
    "    \"\"\"Loop through article items and parse the title, description, link, and date of\n",
    "    each article, creating a dictionary. Append each dictionary to a list.\"\"\"\n",
    "    \n",
    "    all_articles = []\n",
    "    for article in articles:\n",
    "        article_title = article.find('title').text\n",
    "        article_link = getattr(article.find('link'), 'text', None)\n",
    "        article_desc = getattr(article.find('description'), 'text', None)\n",
    "        article_published = getattr(article.find('pubDate'), 'text', None)\n",
    "        all_articles.append({\n",
    "            'title':article_title,\n",
    "            'link':article_link,\n",
    "            'description':article_desc,\n",
    "            'published':article_published\n",
    "        })\n",
    "    return all_articles\n",
    "\n",
    "# Initialize a list to store parsed results from each <rss> section\n",
    "all_parsed_results = []\n",
    "\n",
    "# Split the XML string into individual <rss> sections\n",
    "rss_sections = appended_xml.split('</rss>')[:-1]\n",
    "\n",
    "# Iterate through each <rss> section and parse it\n",
    "for rss_section in rss_sections:\n",
    "    # Add the opening <rss> tag back to form a valid XML document\n",
    "    rss_section = '<rss>' + rss_section\n",
    "    soup = BeautifulSoup(rss_section, 'xml')\n",
    "\n",
    "    # Parse and process the content within the <rss> section as needed\n",
    "    # Example: Extract items, titles, descriptions, etc.\n",
    "    items = soup.find_all('item')\n",
    "\n",
    "    # Append the parsed result to the list\n",
    "    all_parsed_results.append(items)\n",
    "    \n",
    "\n",
    "# Combine the parsed results from all <rss> sections as needed\n",
    "# Example: Flatten the list, merge, or process the data further\n",
    "combined_results = [item for items in all_parsed_results for item in items]\n",
    "\n",
    "# Print or process the combined results\n",
    "# print(combined_results)\n",
    "\n",
    "# items_2 = combined_results.findAll('item')\n",
    "articles_dict = getArticles(combined_results)\n",
    "#print(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac4e51c-81b7-43ec-8e48-a6bd722e87d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>description</th>\n",
       "      <th>published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193541</th>\n",
       "      <td>'Hide the decline' revisited</td>\n",
       "      <td>http://www.bbc.co.uk/go/rss/int/news/-/news/sc...</td>\n",
       "      <td>Does Earth's new temperature record hide a dec...</td>\n",
       "      <td>Wed, 02 Nov 2011 11:15:47 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193542</th>\n",
       "      <td>What's happened to Thalidomide babies?</td>\n",
       "      <td>http://www.bbc.co.uk/go/rss/int/news/-/news/ma...</td>\n",
       "      <td>Thalidomide victims search for truth, 50 years on</td>\n",
       "      <td>Thu, 03 Nov 2011 01:18:34 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193543</th>\n",
       "      <td>Colourful remembrances of lost birds</td>\n",
       "      <td>http://www.bbc.co.uk/go/rss/int/news/-/news/ma...</td>\n",
       "      <td>Bright visions of birds that are no longer wit...</td>\n",
       "      <td>Wed, 02 Nov 2011 10:46:25 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193544</th>\n",
       "      <td>7 questions on national animals</td>\n",
       "      <td>http://www.bbc.co.uk/go/rss/int/news/-/news/ma...</td>\n",
       "      <td>Test yourself on birds and beasts used as symbols</td>\n",
       "      <td>Wed, 02 Nov 2011 10:55:50 GMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193545</th>\n",
       "      <td>Five things Scott found in Antarctica</td>\n",
       "      <td>http://www.bbc.co.uk/go/rss/int/news/-/news/ma...</td>\n",
       "      <td>Five strange things Captain Scott found in the...</td>\n",
       "      <td>Wed, 02 Nov 2011 04:44:08 GMT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  \\\n",
       "193541            'Hide the decline' revisited   \n",
       "193542  What's happened to Thalidomide babies?   \n",
       "193543    Colourful remembrances of lost birds   \n",
       "193544         7 questions on national animals   \n",
       "193545   Five things Scott found in Antarctica   \n",
       "\n",
       "                                                     link  \\\n",
       "193541  http://www.bbc.co.uk/go/rss/int/news/-/news/sc...   \n",
       "193542  http://www.bbc.co.uk/go/rss/int/news/-/news/ma...   \n",
       "193543  http://www.bbc.co.uk/go/rss/int/news/-/news/ma...   \n",
       "193544  http://www.bbc.co.uk/go/rss/int/news/-/news/ma...   \n",
       "193545  http://www.bbc.co.uk/go/rss/int/news/-/news/ma...   \n",
       "\n",
       "                                              description  \\\n",
       "193541  Does Earth's new temperature record hide a dec...   \n",
       "193542  Thalidomide victims search for truth, 50 years on   \n",
       "193543  Bright visions of birds that are no longer wit...   \n",
       "193544  Test yourself on birds and beasts used as symbols   \n",
       "193545  Five strange things Captain Scott found in the...   \n",
       "\n",
       "                            published  \n",
       "193541  Wed, 02 Nov 2011 11:15:47 GMT  \n",
       "193542  Thu, 03 Nov 2011 01:18:34 GMT  \n",
       "193543  Wed, 02 Nov 2011 10:46:25 GMT  \n",
       "193544  Wed, 02 Nov 2011 10:55:50 GMT  \n",
       "193545  Wed, 02 Nov 2011 04:44:08 GMT  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(articles_dict)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b1d4c10-009c-46fa-ae6f-eb985de69d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193546, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79a9230d-45e3-4bee-9029-8b5c628263c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df.drop_duplicates(subset= 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb8c94e1-525b-4ee2-812f-f9c562cbcfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20102, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afb1a2e9-e435-4b3e-b32b-01cdd44a7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.to_csv(\"../data/bbc_science_and_climate_articles_2010-2023.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
